---
layout: post
title: "S3"
date: 2020-11-23
categories: ['AWS']
excerpt_separator: <!--more-->
---

# S3
1. S3 has six storage classes
   
    1. S3 Standard: 4x9 availability
    2. S3 Standarn Infrequent Access (IA): Use when retrieving infrequently like once a month: 99.9 availability
    3. S3 one Zone IA : Same as #2 only in single AZ: 99.5 availability
    4. S3 Intelligent Tiering: Based on AI move objects from Standard to IA
    5. S3 Galcier: Archive data; retrieval time from min to hours
    6. S3 Glacier Deep Archive: Retrival time uptop 12 hours

    You can select different storage classes for objects in same bucket.

| Storage Class | Retrieval Latency | Pricing (cents/GB) |Minimum Storage Duration Charge | Min Object size|
|-------|:------:|:------------------------:|:------------------------:|:------------------------:|
| S3 Standard | ms| 2.3 | NA | NA|
| S3 IT | ms| 1.25 | 30 | NA |
| S3 IA | ms| 2.3 | 30 | 128 KB |
| S3 IA 1 zone | ms| 2.3 |30 | 128 KB |
| S3 Archive | ms| Min to hours|30 | 128 KB |
| S3 Deep Archive | ms| 12 + Hrs |30 | 128 KB |

2. Pricing is affected by:
    
    1. Cross Region Replication
    1. .023 / GB / month
    4. Network in via internet free
    5. Network out to any AWS service in same region free
    6. Network Out to internet: .09/GB
    7. Put requests: .005/1000
    8. Get requests: .004/10000

    ## GB-Month

    If you are using 100GB of storage for 15 days and 200 for another 15 days, this is how the pricing is calculated:

    1. Convert the GB's to GB Hours Used = (100 * 15 * 24) + (200 * 15 * 24) = 36000+72000 = 108000 GB hours
    2. Convert the GB hours to GB months = 108000 /744 (hours in months) = 145.2
    3. Multiply this by the rate which is tiered = 145.2 * .023 = 3.38 USD
    


3. Things to remember:
    1. S3 is object based storage and not block based storage.
    2. MFA can be enabled on S3 objects
    3. Eventual consistency for PUT but real time read write consistency for POST of objects
    4. File size 0-5 TB
    5. Unlimited storage
    6. Universal namespace across all regions: https://<region>.amazonaws.com/<bucketname>
    7. S3 access logs can be enabled to be stored in same account or in other account
    8. 

6. S3 key properties:

    1. Key value
    2. Version ID
    3. Metadata
    4. Subresources; ACL and torrent

7. S3 Provisioned Capacity Unit: Single unit costs $100 per month and gurantee 3 expedite retrivals every 5 mins with 150 MB/s throughput

8.  Transfer accelaration: 

    1. copies data to edge location of s3 using cloudfront. Once the transfer accelaration is enables, you get accelarated endpoint of S3 bucket

    2. The files are copied to the edge locations first and then they are synced to S3 bucket.

9. S3 Permissions:

    1. Bucket level policy
        
        a. Bucket Policies: For bucket level perms
        b. ACL: For object level permissions
    2. Object level policy
    3. IAM user and role policy

11. S3 encryption
    
    At rest:
    
    1. S3 managed keys - Server Side Encryption SSE - s3
    2. AWS Key Management Service - SSE - KMS
    3. SSE with custmer provides keys

    S3 encryption can be Done at object level and at bucket level


12. S3 versioning:
    
    1. Once enabled, cannot be disabled, only can be suspended. To disable, have to recreate the bucket
    2. Supports MFA for versioning.
    3. Deleting parent does not delete the version. You have to do a perm delete of all version. If you just delete the parent, it will just put delete marker.

13. Public access: 

    1. To access an object publicly, you have to make the bucket public and then the object public
    2. To access an public object version, you have to make it's version public explicitly. If the parent version is public, that does not mean that thre subsequent versions are public

14. S3 Lifecycle Management

    1. Can be set for all or prefix
    2. Can be used along with versioned of objects both current and old.
    3. Can be used to move objects between different tiers in automated fashion.


15. S3 object Lock
    
    1. Can be used for WORM model. 
    2. This can lock object once uploaded. This can be used for compliance
    3. Two modes: 
        
        1. Governance mode: YOu can specify which users can overide the locks
        2. Compliance mode: No one can overided the lock even root for the retention period
            Retention Period is the duration for which and object is going to be locked.

    4. Can be on bucket or on individual objects
16. S3 Performance

    1. S3 Prefix are nothing but the middle part between the bucket name and the object name.
    2. Per Prefix you can have: 
        
        3500 POST/PUT/COPY/DELETE request
        5500 GET/HEAD requests
    3. Thus, more prefix better performance
    4. When S3 - KMS is enabled, each upload and download will result in API call to AWS KMS service.  There is hard limit for number of API calls per region per account (5000, 10000 , 30000etc)
    5. S3 multipart upload: Good for files over 100 MB and requrired for files more than 5GB
    6. S3 byte range fetches: Used to parallize download. Can be used to just get few bytes of file


17. S3 Select is used only to query a single file stored in structured format such as JSON, CSV and Parquet. You are charge only for the selected data being transferred. 

18. S3 replication:

    1. Needs versioning enabled on both sides of the bucket.
    2. Can be done in same region accross different accounts or accross different regions different accounts
    3. Existing files not replicate but all subsequest files/version will be automatically replicated.
    4. Delete markers and versions not replicated.

19. Data Sync:

    1. Used to move large amount of data from on-promises to AWS and between various aws storage services
    2. For on prem, you need to install data sync agent on EC2. 
    3. Can be used to replicate between AWS EFS, S3, FSx.
    4. 

20. Cloudfront is AWS managed CDN. It is used to store the static content to the edge location. Same concepts of CDN , origin, Edge locations and distribution. 

    1. Supports two type of distributions:

        1. RTMP protocol no longer supported.
        2. Web Distributions - Typically used for websites

    1. Invalidating cache costs. Use source versioning instead. This means everytime you update the static content you have to update your application code to point to it.
    2. Edge locations are also used for S3 transfer accelaration. Thus, you can upload data to the edge location making them writable. Thus edge location are just no readonly you can write to them as well. 


21. AWS Snowball: It is a 50-10 TB physical transfer devide. It is used to move large amount of data into and out of AWS. It uses industry approved TPM security modules.
22. AWS Snowball Edge: 100 TB of storage with compute capability and clustering capability with other snoball edge devices. Used to remote location storage and compute.
23. AWS Snowmobile: Exabyte scale transfer service used to move extreme large amount of data to AWS. Often used to move video libraries or even entire data center. 

24. Storage Gateway: This is physical or virtual device use to replicate data from datacenter to AWS (S3). It is of three types: 

    1. File GW: Files stored in s3 over SMB and NFS. This is including permissions.
    2. Volume GW: Disk volumes copied to S3 over iSCSI protocol to AWS EBS.
        Stored Volumes: Data is stored locally entirely and then replicated to AWS EBS async.
        Cacched volumes: Only partially data is stored on premises.

    3. VTL: Leverages existing Tape based backup mechanism to back data to VTL.

25. Athena:
    
    1. Interactive query service which enables you to analyze and quyery data located in S3 using standard SQL. To summarize, you can turn S3 into large database.

26. Macie:
    1. Security service which uses Machine Learning and NFP (AI) to recognise if your S3 objects contain sensitive data.
    2. It uses NLP (natural Language Processing). Great for PCI -DSS.

27. Static Website: 
    1. Create S3 bucket.
    2. Make it public.
    3. Add index.html and error.html
    4. Enable statics website hosting and give index.html and error.html pages as standard pages
    5. Add s3 bucket policy using PEAR.
    6. Access the bucket using url: https://<bucketname>.s3-website-<region>amazonaws.com
    